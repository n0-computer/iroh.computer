import { BlogPostLayout } from '@/components/BlogPostLayout'
import Image from 'next/image'

export const post = {
  draft: false,
  author: 'RÃ¼diger Klaehn',
  date: '2024-06-11',
  title: 'Iroh content discovery',
  description:
    'How to discover iroh nodes by node id, using the Mainline Distributed Hash Table (DHT)',
}

export const metadata = {
  title: post.title,
  description: post.description,
}

export default (props) => <BlogPostLayout article={post} {...props} />

## Problem

The blobs layer of iroh deals with efficient verified streaming of
content-addressed data. Basically if you have a blake3 hash of some data,
whether it is a kilobyte or a terabyte, and you know an iroh node
that has that data, you can ask that node for the data, and stream it
in a very efficient way. Every few kilobytes, the data coming from the node
will be incrementally verified, so you can stream from an
untrusted source.

In many cases this is already quite useful. E.g. in case of the sendme tool,
you send data from one node (the sender) to another
node (the receiver) in a verified way.

In the case of iroh documents, you can make the very reasonable assumption
that people that have the document are good candidates for downloading the
data that is contained in the document.

And you have a big server like a home NAS behind a firewall,
there is also no question where the data is coming from.

But there are a number of use cases where you don't have any of this.
E.g. you want to globally publish some data and have it available
as long as there is a single node worldwide that still has the data,
even if the original publisher is long gone.

In that case you need a system that, given a hash for some content,
gives you a number of nodes that might be good candidates
to download the data for the node from.

It might come as a surprise that iroh does not have such a system at
this time. The reason for this is that while the API enabled
by this functionality is deceptively simple, implementing such a system
on a global scale is an extremely challenging proposition.

We are pretty sure that we could come up with a system that would
work well at a small to medium scale. but we don't want to have a system built
in that will either become unresponsive as the number of iroh nodes grows, or
causes unsustainable infrastructure costs.

# Possible solutions

## Context

Often you don't just have a hash, but a rough idea in what context the data
corresponding to that hash has been created. In all these cases it is very
inefficient to go to a global system that has to keep track of trillions of
pieces of content, finding the needle in a giant worldwide haystack.

Instead it is much better to search the set of pieces of content related to the
context.

An example of this is to ask only nodes that have an iroh document for content
related to that document.

## DHTs

### Mainline

One of the oldest existing systems for truly global content discovery is the
bittorrent mainline DHT. It's most fundamental function is to associate a SHA1
*infohash* with a number of providers identified by ipv4 or ipv6 addresses and
ports.

We are already making use of the bittorrent mainline DHT for fully peer to peer
discovery of node addresses from ed node ids.

### Other DHTs

There are a large number of p2p projects that develop their own DHTs. Most notably
possibly the ipfs DHT, which has been renamed to
[amino DHT](https://blog.ipfs.tech/2023-09-amino-refactoring/), and the hypercore
DHT that is now part of the holepunch project.

The problems with these DHTs is that they don't operate at sufficient scale to
be really useful, and that they have some issues with stability and performance.

## Trackers

A very straightforward solution is to have a server that just keeps track of 
who has content. Everybody who has content announces to this server, and whenever
you need content you ask a tracker who has it. Such a tracker needs just a very
simple key value store to store very little information (just a set of node ids
for each piece of content), so you could plausibly run a tracker for billions or
trillions of pieces of content on a single current server.

However, there are a few problems. First of all, if you have a piece of content
with no context information attached, which tracker do you ask? Second, a tracker
is a single point of failure.

## DNS

