import { BlogPostLayout } from '@/components/BlogPostLayout'
import {ThemeImage} from '@/components/ThemeImage'

export const post = {
  author: 'RÃ¼diger Klaehn',
  date: '2026-03-15',
  title: 'QUIC packet rejection in practice',
  description:
    'Putting QUIC packet rejection to the test with a real iroh endpoint.',
}

export const metadata = {
  title: post.title,
  description: post.description,
}

export default (props) => <BlogPostLayout article={post} {...props} />

In the previous [blog post](/blog/quic-packet-rejection) we have explored the
mechanisms QUIC uses to reject packets.

Now we want to see how this works in practice.

# A test handler

We start with an iroh endpoint that serves a simple echo protocol and counts completed requests.

This endpoint runs as a standalone binary. You can configure the endpoint id in the usual way with an `IROH_SECRET` environment variable. The binary has a cli argument to shut down after either n total or n completed requests.

It will also measure its own CPU time on shutdown, using the [cpu-time](https://docs.rs/cpu-time/) crate.

Using this we can measure not just the wallclock exeuction time but also how much load the server was under.

# Reference case: just valid requests

To establish a baseline, we just do a number of echo requests in parallel and look at the CPU usage and wallclock time:

```
echo: 100 accepted,   0 rejected,   0 closed  (client: 45.55ms | server: 133.88ms cpu,      747 ops/s, 47.77ms wall)
```

So each *cpu core* can handle 747 complete new handshakes and echo requests per second in terms of raw CPU time.

Note that these are fresh connection attemps. Also this is with 100 clients on running on the same machine, so the absolute number isn't that meaningful. We mostly care about relative numbers.

Now let's imagine our poor echo service is under load or under a denial of service attack. We will see at which places we can reject incoming connection attempts, and how many we can reject.

# Rejection hooks

![Connection filter pipeline](/blog/quic-packet-rejection-in-practice/connection-filter.svg)

## Direct requests

The first incoming datagram will contain an initial packet without retry token. At this point we don't know if the sender addresss is valid. This is the first opportunity to reject.

We could reject here based on the unverified sender address, or just to shed load.

```rust
```

The options here are ignore (silently dropping the connection attempt), retry (instruct the sender to retry with a token), and reject.

```
ignore addr:   0 accepted, 100 rejected,   0 closed  (client:   5.01s | server:  1.49ms cpu,    67204 ops/s,  9.65ms wall)
reject addr:   0 accepted, 100 rejected,   0 closed  (client:  9.95ms | server:  9.73ms cpu,    10276 ops/s, 13.05ms wall)
retry + reject:   0 accepted, 100 rejected,   0 closed  (client: 36.45ms | server: 14.10ms cpu,     7095 ops/s, 39.56ms wall)
```

Unsurprisingly, ignore is fastest at 67204 ops/s per core. But also not very useful except for unconditional load shedding.

Reject is a bit more polite, but for spoofed sender addresses it might not even reach the sender.

Retry+reject is the first rejection mode that is genuinely useful for selective filtering or rate limiting. After a retry we know the source address is correct, so we can e.g. rate limit by ip address or even filter by region.

Even retry+reject is much faster than going through with the request at ~7095 ops/s per core.

The last opportunity to reject direct requests is to reject based on the negotiated ALPN. This is useful not so much in a denial of service scenario, but if you serve different ALPNs and want to prioritize one in times of high load.

```
reject alpn:   0 accepted,   0 rejected, 100 closed  (client: 34.33ms | server: 81.50ms cpu,     1227 ops/s, 38.66ms wall)
```

In case of an echo service that is extremely cheap, this does not help much compared to completing the request. But in case of more expensive requests it would help a lot to prioritize.

## Requests via relays

For requests via relays we don't have a source address that we can filter or rate limit on. What we have instead is the endpoint id as reported by the relay.

This id can be relied on only if the relay isn't lying to us, so it is a bit similar to an unverified source ip address. But it is much harder for an attacker to arrange for a compromised relay than to just forge the sender in UDP datagrams, so it is a bit more useful for filtering.

```
reject endpoint id:   0 accepted, 100 rejected,   0 closed  (client: 14.12ms | server:  4.88ms cpu,    20509 ops/s, 27.25ms wall)
```

Once the handshake is complete, we have the opportunity to filter both on the *verified* remote endpoint id and the negotiated ALPN.

```
reject alpn:   0 accepted,   0 rejected, 100 closed  (client: 48.22ms | server: 69.35ms cpu,     1442 ops/s, 58.18ms wall)
```

This helps a bit even in case of the extremely cheap echo requests.

# Combining valid requests with spam

The last step is to try to communicate with our echo service while it is being subjected to a denial of service attack.

```
  echo: 100/112 accepted, 12 rejected
  server wall time:   2.80s
  server cpu time:  450.74ms
  spam packets sent: 11200
  server incoming:   2545
  server filtered:   100
  server completed:  100
  effective spam/request: 112:1
```

In this test we spam the echo service binary with 100 fake `ClientHello` per valid echo request. Each `ClientHello` is valid, so it will cause the endpoint to respond with a ServerHello that will be ignored.

Not all spam packets actually arrive at the server, several are dropped by the operating system (it is allowed to do this for UDP datagrams).

But even so, we got 25 times as many fake requests arriving at the endpoint than real requests, and nevertheless all valid echo requests are getting through in a reasonable time (4.5ms of server core time per request).

# Firewall 